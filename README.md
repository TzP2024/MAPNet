# Efficient RGB-D Co-Salient Object Detection via Modality-Aware Prompting
RGB-D co-salient object detection (RGB-D Co-SOD) is aiming at identifying and detecting salient objects in a set of correlated images and depth maps. The majority of existing RGB-D Co-SOD approaches comprehensively fine-tune the dual-stream encoderâ€“decoder framework and fuse the RGB and depth features through a complex feature-fusion strategy that is expensive to train owing to the large number of parameters that are updated during the feature extraction and fusion processes. Therefore, this study proposes a simple and effective modality-aware prompting network (MAPNet) for efficient RGB-D Co-SOD training. MAPNet trains an RGB-D Co-SOD through two approaches, namely modal fusion and consensus feature extraction, using a multimodal prompt generator (MPG) and consensus feature extraction module (CFEM), respectively. Specifically, the MPG module guides the depth features in the fine-tuned backbone network from the RGB features obtained in the frozen backbone network for fusion in hyperbolic spaces to generate multilevel modal cues that are subsequently injected into the fine-tuned backbone network for efficient modal fusion. CFEM uses RGB features to generate an image salient prior, combines the salient prior with the highest level of fusion features to obtain the central point, and uses the salient features closer to the central point as the consensus features of the image group. In addition, contrast loss is introduced to separate the synergistic salient features from the non-synergistic salient features to obtain pure co-salient features. The trained MAPNet obtained state-of-the-art performance on three baseline datasets (RGBD CoSal1k, RGBD CoSal150, and RGBD CoSeg183).

# Framework Overview
